# Sana / Sana Sprint / Sana Video â€“ Research Experiments

This repo is a **research playground** for experiments on top of `SANA` and `Sana Sprint` diffusion models.  
It currently includes **prototype code**, **one-off experiments**, and **work-in-progress training scripts** (LoRA, DreamBooth-style tuning, zoom conditioning, control net etc).



## Sana 1.5 + SigLIP: img2img with Zoom Controller

This experiment turns **Sana 1.5** into an **img2img model with a continuous zoom controller**, driven by **SigLIP** image embeddings.

At a high level:

- Take a collection of **GLB objects** and render them offline at multiple zoom levels (e.g. `zoom_0`, `zoom_1`, â€¦, `zoom_N`) to create a 2D image dataset.
- Encode the **zeroâ€‘zoom render** (`zoom_0`) with **SigLIP**, then pass that embedding through a small MLP to get an **object token**.
- Map the scalar zoom value `z âˆˆ [0, 1]` through a separate MLP to obtain a **zoom token**.
- Append both tokens (object + zoom) to the already cached **text encoding** (sometimes an objectâ€‘specific description, sometimes a generic class prompt), and fineâ€‘tune **only the Sana 1.5 transformer** with **LoRA**.
- At inference, given a **zeroâ€‘zoom reference image**, a **target zoom value** (and optional text description), the model generates a **single image at the requested zoom**, without reâ€‘rendering the GLB.

### Method Overview

<p align="center">
  <img
    src="assets/siglip_zoom_sana/method_overview_nanobanana.png"
    alt="SigLIPâ€“zoom method overview"
    width="1024"
  />
</p>

> ðŸ“ **Note**  
> This illustration was generated using nano banaÖ¿ and is a slightly idealized view of the method. A few things are not strictly accurate:
> - LoRA is applied **only to the Sana 1.5 transformer**.
> - The SigLIP image embedding and scalar zoom value are mapped by two different MLPs and then appended as **two separate tokens** to the already cached text encoding (sometimes an object-level text embedding, sometimes a class prompt).
> - GLB rendering happens **offline** to build a dataset of multi-zoom images; the diffusion model never sees raw GLB assets directly at train or inference time.
> - The model generates **one image per call**: you input a zero-zoom object and a target zoom (plus optional description), and it generates the object at the requested zoom.

1. **Offline GLB rendering**  
   - Render each GLB object at multiple zoom levels (`zoom_0 â€¦ zoom_N`) to build a 2D dataset.
2. **SigLIP object token**  
   - Take the zeroâ€‘zoom image (`zoom_0`), encode it with SigLIP, and feed the embedding into a small MLP to produce an **object token**.
3. **Zoom token (scalar â†’ embedding)**  
   - Normalize the desired zoom value `z âˆˆ [0, 1]` and pass it through a separate MLP to obtain a **zoom token**.
4. **Text + tokens conditioning**  
   - Run the text encoder on an objectâ€‘level or classâ€‘level prompt, then append the **object token** and **zoom token** as two additional tokens to the cached text encoding.
5. **Img2img Sana 1.5**
   - The LoRAâ€‘tuned transformer learns to keep the object appearance consistent while changing only the zoom level.


### Reference & Zoom Sweep Example

| Reference zeroâ€‘zoom view | Zoom controller video                                                            |
|--------------------------|----------------------------------------------------------------------------------|
| <img src="assets/siglip_zoom_sana/referance_zero_zoom_example.jpeg" alt="Reference zero-zoom view" width="256" height="256"/> | ![Zoom controller video example](assets/siglip_zoom_sana/video_example_zoom.gif) |

The table above shows a **single reference render** (left) and a **zoom sweep** generated by the zoomâ€‘controlled Sana 1.5 model (right), where only the zoom value is changed while the object identity and style remain consistent.

## Sana-Video Tom & Jerry Experiment

This experiment fine-tunes **SANA-Video (2B)** with **LoRA** on a dataset of _Tom and Jerry_ clips resized to **224Ã—224**.  
The base model is originally trained for higher resolutions (e.g. 480p), so its **zero-shot** generations at 224Ã—224 are:

-- off-style compared to classic 2D slapstick cartoons  
-- less consistent in line art, color palette, and motion
 
---

### Training Setup

#### Objective
[![Hugging Face â€“ sanavideo-tomjerry-lora-r16-v1](https://img.shields.io/badge/HuggingFace-sanavideo--tomjerry--lora--r16--v1-ffcc4d?style=for-the-badge&logo=huggingface&logoColor=black)](https://huggingface.co/AmitIsraeli/sanavideo-tomjerry-lora-r16-v1)  
[![Weights & Biases â€“ Tom & Jerry Runs](https://img.shields.io/badge/Weights_&_Biases-Tom_%26_Jerry%20Runs-2c8ebb?style=for-the-badge&logo=weightsandbiases&logoColor=white)](https://wandb.ai/amit154154/sana-video-tomjerry)

- Start from **base SANA-Video 2B**  
- Freeze everything except **LoRA adapters on all linear layers in the diffusion transforme**  
- Train with a **single class prompt** describing the Tom & Jerry world  
- Resolution: **224Ã—224**  
- Dataset: curated Tom & Jerry-style clips

#### Key Hyperparameters

| Component          | Value                                |
|--------------------|--------------------------------------|
| Base model         | `SANA-Video_2B_480p_diffusers`       |
| Resolution         | 224 Ã— 224                            |
| Batch size         | 8                                    |
| Optimizer          | AdamW / AdamW8bit (LoRA params only) |
| Learning rate      | 2e-4                                 |
| LoRA rank          | 16                                   |
| LoRA alpha         | 32                                   |
| LoRA dropout       | 0.1                                  |
| Training objective | Flow Matching (velocity prediction)  |
| Steps              | 10,000                               |
| clip length        | 81 frames                            |



---

### Class Prompt (Style Anchor)

The first training run used a **single class prompt** to define the style:

```text
A vintage slapstick 2D cartoon scene of a grey cat chasing a small brown mouse in a colorful house, Tom and Jerry style, bold outlines, limited color palette, exaggerated expressions, smooth character motion.
```

---

### Training Progression (Class LoRA)

During training, videos were generated every few steps with a fixed seed and CFG to visualize how the LoRA gradually pulls SANA-Video toward the Tom & Jerry domain.

All of these samples live in:

- `assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4`

#### Checkpoint Comparison (Same Seed, Same Prompt)

| Step   | Video |
|--------|-------|
| Base   | ![Base](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_base_seed69420.gif) |
| 100    | ![Step 100](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step000100_seed69420.gif) |
| 1,000  | ![Step 1k](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step001000_seed69420.gif) |
| 2,000  | ![Step 2k](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step002000_seed69420.gif) |
| 5,000  | ![Step 5k](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step005000_seed69420.gif) |
| 7,500  | ![Step 7.5k](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step007500_seed69420.gif) |
| 10,000 | ![Step 10k](assets/tom_and_jerry_all_assets/tomjerry_video_class_lora16_seed69420_cfg4/tomjerry_lora_step010000_seed69420.gif) |

when all examples are from seed 69420 and the same class prompt  

---

### Text-Conditioned Experiments

After the class-LoRA training, the next step was to test how much **text conditioning ability** remained:

- Same LoRA, **different prompts** for different scenarios.
- Goal: see whether the model can respond to new text instructions **without losing** the learned Tom & Jerry style.



| Full prompt | Summary to prompt | Video |
|-------------|-------------------|-------|
| A vintage slapstick 2D cartoon scene of a grey cat chasing a small brown mouse in a colorful house, Tom and Jerry style, bold outlines, limited color palette, exaggerated expressions, smooth character motion. | Class baseline (same style anchor as training prompt) | <img src="assets/tom_and_jerry_all_assets/fine_tuned_class_textcond/tomjerry_classcond_seed32420.gif" alt="Class baseline" width="256"/> |
| A classic slapstick 2D cartoon scene of a grey cat sneaking after a small brown mouse in a cozy living room at night, Tom and Jerry style, bold outlines, warm lamp lighting, deep shadows, exaggerated sneaking poses, smooth tiptoe animation. | Night living room sneaking (tests lighting and sneaking motion) | <img src="assets/tom_and_jerry_all_assets/fine_tuned_class_textcond/tomjerry_at_night_seed32420.gif" alt="At night" width="256"/> |
| A retro slapstick 2D cartoon scene of a grey cat and small brown mouse chasing each other up and down a staircase in a big old mansion, Tom and Jerry style, bold outlines, muted vintage colors, dust clouds, stretched limbs, smooth looping motion. | Vertical staircase chase in an old mansion | <img src="assets/tom_and_jerry_all_assets/fine_tuned_class_textcond/tomjerry_vertical_chase_oldbuilding_32420.gif" alt="Vertical chase" width="256"/> |
| A vintage slapstick 2D cartoon scene of a grey cat chasing a small brown mouse in a colorful house, Tom and Jerry style, bold outlines, limited color palette, exaggerated expressions, smooth character motion. tom is eating a pizza. | Tom eating pizza indoors (object + action added) | <img src="assets/tom_and_jerry_all_assets/fine_tuned_class_textcond/tomjerry_tom_eatingpizza_32420.gif" alt="Tom eating pizza" width="256"/> |
| A vintage slapstick 2D cartoon scene of a grey cat being outsmarted by a small brown mouse in a sunny backyard, Tom and Jerry style, bold outlines, vibrant green grass and blue sky, exaggerated pranks, falling objects, smooth expressive animation. | Prank outside in a sunny backyard | <img src="assets/tom_and_jerry_all_assets/fine_tuned_class_textcond/tomjerry_prank_outside_32420.gif" alt="Prank outside" width="256"/> |
> ðŸ“ **Note**  
> All text-conditioned videos are generated using the same seed (`32420`).

### Metrics & Evaluation (Template)

TBD


### Next Steps:

1.	Characterize training stability: Run a 15k-step LoRA-64 experiment (LR = 1e-4 with warmup) to study the early-step loss explosion and compare dynamics against the original LoRA-16 run.
2.	VLM-driven control & labeling: Use a visionâ€“language model to auto-label clips with richer descriptions and control tags, and design a prompt paradigm for video â†’ text â†’ video conditioning.
3.	Model compression & sampling study: Systematically evaluate distillation, pruning, and different sampling-step budgets, measuring their impact on quality and downstream metrics.